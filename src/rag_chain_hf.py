# rag_chain_hf.py
import os
# ‚úÖ ‡πÉ‡∏ä‡πâ HuggingFaceEmbeddings ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
import config

# ‚úÖ ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Path ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏ô ingestion_hf.py
DB_PATH_HF = "./chroma_db_hf"

def start_chat():
    print("üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏£‡∏∞‡∏ö‡∏ö Chatbot (HF Embeddings + Gemini LLM)...")

    # --- 1. Setup Models ---
    # ‡πÉ‡∏ä‡πâ Embedding ‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ï‡∏≠‡∏ô Ingest
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    
    # LLM ‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ Gemini (‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡πÅ‡∏Ñ‡πà‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤ Embedding)
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", google_api_key=config.GEMINI_API_KEY)

    # --- 2. Load Vector Store ---
    if not os.path.exists(DB_PATH_HF):
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà {DB_PATH_HF} ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏±‡∏ô ingestion_hf.py ‡∏Å‡πà‡∏≠‡∏ô")
        return

    vectorstore = Chroma(persist_directory=DB_PATH_HF, embedding_function=embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

    # --- 3. Create Chain ---
    system_prompt = (
        "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° "
        "‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö (Context) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ "
        "‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö ‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏Å‡∏ï‡∏≤‡∏°‡∏ï‡∏£‡∏á‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö ‡∏≠‡∏¢‡πà‡∏≤‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏ï‡πà‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏≠‡∏á "
        "‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢"
        "\n\n"
        "{context}"
    )

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}"),
    ])

    question_answer_chain = create_stuff_documents_chain(llm, prompt)
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)

    # --- 4. Chat Loop ---
    print("ü§ñ ‡∏£‡∏∞‡∏ö‡∏ö‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô! (‡∏û‡∏¥‡∏°‡∏û‡πå 'exit' ‡∏´‡∏£‡∏∑‡∏≠ 'quit' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏≠‡∏Å)")
    while True:
        user_input = input("User: ")
        if user_input.lower() in ["exit", "quit"]:
            print("üëã ‡∏ö‡πä‡∏≤‡∏¢‡∏ö‡∏≤‡∏¢!")
            break
        
        try:
            response = rag_chain.invoke({"input": user_input})
            print(f"Bot: {response['answer']}")
        except Exception as e:
            print(f"‚ö†Ô∏è ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")

if __name__ == "__main__":
    start_chat()